{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c024bfa4-1a7a-4751-b5a1-827225a3478b",
   "metadata": {
    "id": "c024bfa4-1a7a-4751-b5a1-827225a3478b"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b8c870-fb72-490e-8916-d8129bd5d1ff",
   "metadata": {
    "id": "58b8c870-fb72-490e-8916-d8129bd5d1ff"
   },
   "source": [
    "# Appendix E: Parameter-efficient Finetuning with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b7e01c2-1c84-4f2a-bb51-2e0b74abda90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b7e01c2-1c84-4f2a-bb51-2e0b74abda90",
    "outputId": "316166b4-027a-4756-e9b4-fe88ae75dd4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.6\n",
      "numpy version: 2.0.2\n",
      "tiktoken version: 0.11.0\n",
      "torch version: 2.8.0\n",
      "tensorflow version: 2.20.0\n",
      "pandas version: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\",\n",
    "        \"numpy\",\n",
    "        \"tiktoken\",\n",
    "        \"torch\",\n",
    "        \"tensorflow\", # For OpenAI's pretrained weights\n",
    "        \"pandas\"      # Dataset loading\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21532056-0ef4-4c98-82c7-e91f61c6485e",
   "metadata": {
    "id": "21532056-0ef4-4c98-82c7-e91f61c6485e"
   },
   "source": [
    "## E.1 Introduction to LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66edc999-3d91-4a1c-a157-9d056392e8d8",
   "metadata": {
    "id": "66edc999-3d91-4a1c-a157-9d056392e8d8"
   },
   "source": [
    "- No code in this section\n",
    "- Low-rank adaptation (LoRA) is a machine learning technique that modifies a pretrained model to better suit a specific, often smaller, dataset by adjusting only a small, low-rank subset of the model's parameters\n",
    "- This approach is important because it allows for efficient finetuning of large models on task-specific data, significantly reducing the computational cost and time required for finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb75b5d-d59c-4948-821a-1594a5883dc1",
   "metadata": {
    "id": "5bb75b5d-d59c-4948-821a-1594a5883dc1"
   },
   "source": [
    "- Suppose we have a large weight matrix $W$ for a given layer\n",
    "- During backpropagation, we learn a $\\Delta W$ matrix, which contains information on how much we want to update the original weights to minimize the loss function during training\n",
    "- In regular training and finetuning, the weight update is defined as follows:\n",
    "\n",
    "$$W_{\\text{updated}} = W + \\Delta W$$\n",
    "\n",
    "- The LoRA method proposed by [Hu et al.](https://arxiv.org/abs/2106.09685) offers a more efficient alternative to computing the weight updates $\\Delta W$ by learning an approximation of it, $\\Delta W \\approx AB$.\n",
    "- In other words, in LoRA, we have the following, where $A$ and $B$ are two small weight matrices:\n",
    "\n",
    "$$W_{\\text{updated}} = W + AB$$\n",
    "\n",
    "- The figure below illustrates these formulas for full finetuning and LoRA side by side"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a7419d-cae9-4525-bb44-1641f6ef4f3b",
   "metadata": {
    "id": "a8a7419d-cae9-4525-bb44-1641f6ef4f3b"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/appendix-e_compressed/lora-1.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edd43c9-8ec5-48e6-b3fc-5fb3c16037cc",
   "metadata": {
    "id": "4edd43c9-8ec5-48e6-b3fc-5fb3c16037cc"
   },
   "source": [
    "- If you paid close attention, the full finetuning and LoRA depictions in the figure above look slightly different from the formulas I have shown earlier\n",
    "- That's due to the distributive law of matrix multiplication: we don't have to add the weights with the updated weights but can keep them separate\n",
    "- For instance, if $x$ is the input data, then we can write the following for regular finetuning:\n",
    "\n",
    "$$x (W+\\Delta W) = x W + x \\Delta W$$\n",
    "\n",
    "- Similarly, we can write the following for LoRA:\n",
    "\n",
    "$$x (W+A B) = x W + x A B$$\n",
    "\n",
    "- The fact that we can keep the LoRA weight matrices separate makes LoRA especially attractive\n",
    "- In practice, this means that we don't have to modify the weights of the pretrained model at all, as we can apply the LoRA matrices on the fly\n",
    "- After setting up the dataset and loading the model, we will implement LoRA in the code to make these concepts less abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7017a2-32aa-4002-a2f3-12aac293ccdf",
   "metadata": {
    "id": "8c7017a2-32aa-4002-a2f3-12aac293ccdf"
   },
   "source": [
    "## E.2 Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c64df-4431-4d27-834d-2bb38a01fc02",
   "metadata": {
    "id": "669c64df-4431-4d27-834d-2bb38a01fc02"
   },
   "source": [
    "- This section repeats the code from chapter 6 to load and prepare the dataset\n",
    "- Instead of repeating this code, one could open and run the chapter 6 notebook and then insert the LoRA code from section E.4 there\n",
    "- (The LoRA code was originally the last section of chapter 6 but was moved to the appendix due to the length of chapter 6)\n",
    "- In a similar fashion, we could also apply LoRA to the models in chapter 7 for instruction finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "def7c09b-af9c-4216-90ce-5e67aed1065c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "def7c09b-af9c-4216-90ce-5e67aed1065c",
    "outputId": "a67a7afe-b401-4463-c731-87025d20f72d"
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from previous_chapters import (\n",
    "    download_and_unzip_spam_data,\n",
    "    create_balanced_dataset,\n",
    "    random_split\n",
    ")\n",
    "# If the `previous_chapters.py` file is not available locally,\n",
    "# you can import it from the `llms-from-scratch` PyPI package.\n",
    "# For details, see: https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n",
    "# E.g.,\n",
    "# from llms_from_scratch.ch06 import (\n",
    "#     download_and_unzip_spam_data,\n",
    "#     create_balanced_dataset,\n",
    "#     random_split\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "# zip_path = \"sms_spam_collection.zip\"\n",
    "# extracted_path = \"sms_spam_collection\"\n",
    "# data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "# try:\n",
    "#     download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "# except (urllib.error.HTTPError, urllib.error.URLError, TimeoutError) as e:\n",
    "#     print(f\"Primary URL failed: {e}. Trying backup URL...\")\n",
    "#     url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/sms%2Bspam%2Bcollection.zip\"\n",
    "#     download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "949bee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = '../../ch06/01_main-chapter-code/sms_spam_collection/SMSSpamCollection.tsv'\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9674d0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Dude how do you like the buff wind.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Tessy..pls do me a favor. Pls convey my birthd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Reminder: You have not downloaded the content ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Got what it takes 2 take part in the WRC Rally...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Shop till u Drop, IS IT YOU, either 10K, 5K, £...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               Text\n",
       "0      0                Dude how do you like the buff wind.\n",
       "1      0  Tessy..pls do me a favor. Pls convey my birthd...\n",
       "2      1  Reminder: You have not downloaded the content ...\n",
       "3      1  Got what it takes 2 take part in the WRC Rally...\n",
       "4      1  Shop till u Drop, IS IT YOU, either 10K, 5K, £..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4b52229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>1</td>\n",
       "      <td>85233 FREE&gt;Ringtone!Reply REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>1</td>\n",
       "      <td>Ur cash-balance is currently 500 pounds - to m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>1</td>\n",
       "      <td>Thanks for your ringtone order, reference numb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>0</td>\n",
       "      <td>We live in the next  &amp;lt;#&amp;gt; mins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>1</td>\n",
       "      <td>1st wk FREE! Gr8 tones str8 2 u each wk. Txt N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label                                               Text\n",
       "1194      1                     85233 FREE>Ringtone!Reply REAL\n",
       "1195      1  Ur cash-balance is currently 500 pounds - to m...\n",
       "1196      1  Thanks for your ringtone order, reference numb...\n",
       "1197      0                We live in the next  &lt;#&gt; mins\n",
       "1198      1  1st wk FREE! Gr8 tones str8 2 u each wk. Txt N..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74c3c463-8763-4cc0-9320-41c7eaad8ab7",
   "metadata": {
    "id": "74c3c463-8763-4cc0-9320-41c7eaad8ab7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from previous_chapters import SpamDataset\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "train_dataset = SpamDataset(\"train.csv\", max_length=None, tokenizer=tokenizer)\n",
    "val_dataset = SpamDataset(\"validation.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
    "test_dataset = SpamDataset(\"test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8681adc0-6f02-4e75-b01a-a6ab75d05542",
   "metadata": {
    "id": "8681adc0-6f02-4e75-b01a-a6ab75d05542"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7335db-e0bb-4e27-80c5-eea11e593a57",
   "metadata": {
    "id": "ab7335db-e0bb-4e27-80c5-eea11e593a57"
   },
   "source": [
    "- As a verification step, we iterate through the data loaders and check that the batches contain 8 training examples each, where each training example consists of 120 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dee6882-4c3a-4964-af15-fa31f86ad047",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4dee6882-4c3a-4964-af15-fa31f86ad047",
    "outputId": "2ae34de1-dd01-4f99-d2c8-ba4dca400754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd7947-7039-49bf-8a5e-c0a2f4281ca1",
   "metadata": {
    "id": "5cdd7947-7039-49bf-8a5e-c0a2f4281ca1"
   },
   "source": [
    "- Lastly, let's print the total number of batches in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "IZfw-TYD2zTj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZfw-TYD2zTj",
    "outputId": "4d19ed61-cf7a-4ec4-b822-c847dd1c5d77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec9aa4a-ffd2-4d9f-a835-cce1059fe604",
   "metadata": {
    "id": "dec9aa4a-ffd2-4d9f-a835-cce1059fe604"
   },
   "source": [
    "## E.3 Initializing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36ebdaf-810e-46a2-9ad9-e017a04051b1",
   "metadata": {
    "id": "f36ebdaf-810e-46a2-9ad9-e017a04051b1"
   },
   "source": [
    "- This section repeats the code from chapter 6 to load and prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02b3a506-3879-4258-82b5-93a5b6bafa74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02b3a506-3879-4258-82b5-93a5b6bafa74",
    "outputId": "b8c9b125-bb52-45d3-8071-fa5054dbf5a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 15:05:06.103658: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: ../../ch05/01_main-chapter-code/gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: ../../ch05/01_main-chapter-code/gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: ../../ch05/01_main-chapter-code/gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: ../../ch05/01_main-chapter-code/gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: ../../ch05/01_main-chapter-code/gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: ../../ch05/01_main-chapter-code/gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: ../../ch05/01_main-chapter-code/gpt2/124M/vocab.bpe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 15:05:11.709203: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 154389504 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "# Alternatively:\n",
    "# from llms_from_scratch.ch04 import GPTModel\n",
    "# from llms_from_scratch.ch05 import load_weights_into_gpt\n",
    "\n",
    "\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "\n",
    "model_dir = '../../ch05/01_main-chapter-code/gpt2'  # 'gpt2'\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=model_dir)  #\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252614cd-7ce6-4908-83e6-3761f519904e",
   "metadata": {
    "id": "252614cd-7ce6-4908-83e6-3761f519904e"
   },
   "source": [
    "- To ensure that the model was loaded corrected, let's double-check that it generates coherent text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b6ce20c-0700-4783-8be0-4cf17c200a7f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b6ce20c-0700-4783-8be0-4cf17c200a7f",
    "outputId": "28ccbca5-8de9-41a0-c093-da00fcbaa91c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import (\n",
    "    generate_text_simple,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "\n",
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8174b31b-1ab5-4115-b01c-245369da5af3",
   "metadata": {
    "id": "8174b31b-1ab5-4115-b01c-245369da5af3"
   },
   "source": [
    "- Then, we prepare the model for classification finetuning similar to chapter 6, where we replace the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e255ce91-d73a-4854-90a4-95804928eb16",
   "metadata": {
    "id": "e255ce91-d73a-4854-90a4-95804928eb16"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=768, out_features=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02e6f057-1383-4ece-8444-0a88e71ac75d",
   "metadata": {
    "id": "02e6f057-1383-4ece-8444-0a88e71ac75d"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is approximately 1.2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# However, the resulting loss values may be slightly different.\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "#\n",
    "# print(f\"Using {device} device.\")\n",
    "\n",
    "model.to(device);  # no assignment model = model.to(device) necessary for nn.Module classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e951cd6-5e42-44d2-b21f-895cb61004fe",
   "metadata": {
    "id": "8e951cd6-5e42-44d2-b21f-895cb61004fe"
   },
   "source": [
    "- Lastly, let's calculate the initial classification accuracy of the non-finetuned model (we expect this to be around 50%, which means that the model is not able to distinguish between spam and non-spam messages yet reliably)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc7dd72c-73a2-4881-ade0-0a9605f1ab8c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc7dd72c-73a2-4881-ade0-0a9605f1ab8c",
    "outputId": "74848515-5a49-4125-fecb-9f4bac23f812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import calc_accuracy_loader\n",
    "# Alternatively:\n",
    "# from llms_from_scratch.ch06 import calc_accuracy_loader\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398a1ec9-e2a1-43d6-bf9f-12ee54b46a7b",
   "metadata": {
    "id": "398a1ec9-e2a1-43d6-bf9f-12ee54b46a7b"
   },
   "source": [
    "## E.4 Parameter-efficient finetuning with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652a4a82-61ef-4d0a-9858-8988e844f12c",
   "metadata": {
    "id": "652a4a82-61ef-4d0a-9858-8988e844f12c"
   },
   "source": [
    "- We begin by initializing a LoRALayer that creates the matrices $A$ and $B$, along with the `alpha` scaling hyperparameter and the `rank` ($r$) hyperparameters\n",
    "- This layer can accept an input and compute the corresponding output, as illustrated in the figure below\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/appendix-e_compressed/lora-2.webp\" width=\"200px\">\n",
    "\n",
    "In code, this LoRA layer depicted in the figure above looks like as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ds9ywjMwvIW",
   "metadata": {
    "id": "2ds9ywjMwvIW"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
    "        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))  # similar to standard weight initialization\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "        self.rank = rank\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Note: The original chapter didn't include the scaling by self.rank\n",
    "        # This scaling is not necessary, but it's more canonical and convenient\n",
    "        # as this lets us compare runs across different ranks without retuning learning rates\n",
    "        x = (self.alpha / self.rank) * (x @ self.A @ self.B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad21faa8-0614-4257-93cd-68952193e14a",
   "metadata": {
    "id": "ad21faa8-0614-4257-93cd-68952193e14a"
   },
   "source": [
    "- In the code above, `rank` is a hyperparameter that controls the inner dimension of the matrices $A$ and $B$\n",
    "- In other words, this parameter controls the number of additional parameters introduced by LoRA and is a key factor in determining the balance between model adaptability and parameter efficiency\n",
    "- The second hyperparameter, `alpha`, is a scaling hyperparameter applied to the output of the low-rank adaptation\n",
    "- It essentially controls the extent to which the adapted layer's output is allowed to influence the original output of the layer being adapted\n",
    "- This can be seen as a way to regulate the impact of the low-rank adaptation on the layer's output\n",
    "- So far, the `LoRALayer` class we implemented above allows us to transform the layer inputs $x$\n",
    "- However, in LoRA, we are usually interested in replacing existing `Linear` layers so that the weight update is applied to the existing pretrained weights, as shown in the figure below\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/appendix-e_compressed/lora-3.webp\" width=\"200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6d5da0-dfce-4808-b89b-29ff333f563f",
   "metadata": {
    "id": "3e6d5da0-dfce-4808-b89b-29ff333f563f"
   },
   "source": [
    "- To incorporate the original `Linear` layer weights as shown in the figure above, we implement a `LinearWithLoRA` layer below that uses the previously implemented LoRALayer and can be used to replace existing `Linear` layers in a neural network, for example, the self-attention module or feed forward modules in an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "127d3a64-8359-4b21-b056-78d58cc75fe8",
   "metadata": {
    "id": "127d3a64-8359-4b21-b056-78d58cc75fe8"
   },
   "outputs": [],
   "source": [
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1145a90-35ff-462c-820b-15483fa5b051",
   "metadata": {
    "id": "e1145a90-35ff-462c-820b-15483fa5b051"
   },
   "source": [
    "- Note that since we initialize the weight matrix $B$ (`self.B` in `LoRALayer`) with zero values in the LoRA layer, the matrix multiplication between $A$ and $B$ results in a matrix consisting of 0's and doesn't affect the original weights (since adding 0 to the original weights does not modify them)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98a6d36-7bc9-434c-a7f1-533f26aff06d",
   "metadata": {
    "id": "e98a6d36-7bc9-434c-a7f1-533f26aff06d"
   },
   "source": [
    "- To try LoRA on the GPT model we defined earlier, we define a `replace_linear_with_lora` function to replace all `Linear` layers in the model with the new `LinearWithLoRA` layers\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/appendix-e_compressed/lora-4.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2b886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_modules of Embedding(50257, 768)>\n",
      "<bound method Module.named_modules of Embedding(1024, 768)>\n",
      "<bound method Module.named_modules of Dropout(p=0.0, inplace=False)>\n",
      "<bound method Module.named_modules of Sequential(\n",
      "  (0): TransformerBlock(\n",
      "    (att): MultiHeadAttention(\n",
      "      (W_query): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (W_key): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (W_value): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (out_proj): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ff): FeedForward(\n",
      "      (layers): Sequential(\n",
      "        (0): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (1): GELU()\n",
      "        (2): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm1): LayerNorm()\n",
      "    (norm2): LayerNorm()\n",
      "    (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (1): TransformerBlock(\n",
      "    (att): MultiHeadAttention(\n",
      "      (W_query): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (W_key): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (W_value): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (out_proj): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ff): FeedForward(\n",
      "      (layers): Sequential(\n",
      "        (0): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (1): GELU()\n",
      "        (2): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm1): LayerNorm()\n",
      "    (norm2): LayerNorm()\n",
      "    (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (2): TransformerBlock(\n",
      "    (att): MultiHeadAttention(\n",
      "      (W_query): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (W_key): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (W_value): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (out_proj): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ff): FeedForward(\n",
      "      (layers): Sequential(\n",
      "        (0): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (1): GELU()\n",
      "        (2): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm1): LayerNorm()\n",
      "    (norm2): LayerNorm()\n",
      "    (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (3): TransformerBlock(\n",
      "    (att): MultiHeadAttention(\n",
      "      (W_query): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (W_key): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (W_value): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (out_proj): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ff): FeedForward(\n",
      "      (layers): Sequential(\n",
      "        (0): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (1): GELU()\n",
      "        (2): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm1): LayerNorm()\n",
      "    (norm2): LayerNorm()\n",
      "    (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (4): TransformerBlock(\n",
      "    (att): MultiHeadAttention(\n",
      "      (W_query): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (W_key): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (W_value): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (out_proj): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ff): FeedForward(\n",
      "      (layers): Sequential(\n",
      "        (0): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (1): GELU()\n",
      "        (2): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm1): LayerNorm()\n",
      "    (norm2): LayerNorm()\n",
      "    (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (5): TransformerBlock(\n",
      "    (att): MultiHeadAttention(\n",
      "      (W_query): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (W_key): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (W_value): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (out_proj): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ff): FeedForward(\n",
      "      (layers): Sequential(\n",
      "        (0): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (1): GELU()\n",
      "        (2): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm1): LayerNorm()\n",
      "    (norm2): LayerNorm()\n",
      "    (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (6): TransformerBlock(\n",
      "    (att): MultiHeadAttention(\n",
      "      (W_query): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (W_key): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (W_value): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (out_proj): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ff): FeedForward(\n",
      "      (layers): Sequential(\n",
      "        (0): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (1): GELU()\n",
      "        (2): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm1): LayerNorm()\n",
      "    (norm2): LayerNorm()\n",
      "    (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (7): TransformerBlock(\n",
      "    (att): MultiHeadAttention(\n",
      "      (W_query): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (W_key): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (W_value): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (out_proj): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ff): FeedForward(\n",
      "      (layers): Sequential(\n",
      "        (0): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (1): GELU()\n",
      "        (2): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm1): LayerNorm()\n",
      "    (norm2): LayerNorm()\n",
      "    (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (8): TransformerBlock(\n",
      "    (att): MultiHeadAttention(\n",
      "      (W_query): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (W_key): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (W_value): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (out_proj): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ff): FeedForward(\n",
      "      (layers): Sequential(\n",
      "        (0): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (1): GELU()\n",
      "        (2): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm1): LayerNorm()\n",
      "    (norm2): LayerNorm()\n",
      "    (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (9): TransformerBlock(\n",
      "    (att): MultiHeadAttention(\n",
      "      (W_query): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (W_key): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (W_value): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (out_proj): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ff): FeedForward(\n",
      "      (layers): Sequential(\n",
      "        (0): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (1): GELU()\n",
      "        (2): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm1): LayerNorm()\n",
      "    (norm2): LayerNorm()\n",
      "    (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (10): TransformerBlock(\n",
      "    (att): MultiHeadAttention(\n",
      "      (W_query): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (W_key): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (W_value): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (out_proj): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ff): FeedForward(\n",
      "      (layers): Sequential(\n",
      "        (0): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (1): GELU()\n",
      "        (2): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm1): LayerNorm()\n",
      "    (norm2): LayerNorm()\n",
      "    (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (11): TransformerBlock(\n",
      "    (att): MultiHeadAttention(\n",
      "      (W_query): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (W_key): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (W_value): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (out_proj): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ff): FeedForward(\n",
      "      (layers): Sequential(\n",
      "        (0): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (1): GELU()\n",
      "        (2): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm1): LayerNorm()\n",
      "    (norm2): LayerNorm()\n",
      "    (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")>\n",
      "<bound method Module.named_modules of LayerNorm()>\n",
      "<bound method Module.named_modules of LinearWithLoRA(\n",
      "  (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (lora): LoRALayer()\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_children():\n",
    "   print(module.named_modules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "WlQZ8ygqzN_g",
   "metadata": {
    "id": "WlQZ8ygqzN_g"
   },
   "outputs": [],
   "source": [
    "def replace_linear_with_lora(model, rank, alpha):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            # Replace the Linear layer with LinearWithLoRA\n",
    "            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
    "        else:\n",
    "            # Recursively apply the same function to child modules\n",
    "            replace_linear_with_lora(module, rank, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c172164-cdde-4489-b7d7-aaed9cc2f5f2",
   "metadata": {
    "id": "8c172164-cdde-4489-b7d7-aaed9cc2f5f2"
   },
   "source": [
    "- We then freeze the original model parameter and use the `replace_linear_with_lora` to replace the said `Linear` layers using the code below\n",
    "- This will replace the `Linear` layers in the LLM with `LinearWithLoRA` layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbe15350-4da9-4829-9d23-98bbd3d0b1a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbe15350-4da9-4829-9d23-98bbd3d0b1a1",
    "outputId": "fd4c208f-854a-4701-d9d3-9d73af733364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters before: 124,441,346\n",
      "Total trainable parameters after: 0\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters before: {total_params:,}\")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters after: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "mLk_fPq0yz_u",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mLk_fPq0yz_u",
    "outputId": "0a93b8fc-05d7-4ace-ee47-e2fc6bdd7d75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable LoRA parameters: 2,666,528\n"
     ]
    }
   ],
   "source": [
    "replace_linear_with_lora(model, rank=16, alpha=16)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable LoRA parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b6819e-ef7a-4f0d-841a-1b467496bef9",
   "metadata": {
    "id": "b8b6819e-ef7a-4f0d-841a-1b467496bef9"
   },
   "source": [
    "- As we can see, we reduced the number of trainable parameters by almost 50x when using LoRA\n",
    "- Let's now double-check whether the layers have been modified as intended by printing the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1711be61-bb2c-466f-9b5b-24f4aa5ccd9c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1711be61-bb2c-466f-9b5b-24f4aa5ccd9c",
    "outputId": "acff8eca-3775-45a2-b62d-032a986ef037"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): LinearWithLoRA(\n",
      "    (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bbc9d7-65ec-4675-bab8-2e56eb0cfb55",
   "metadata": {
    "id": "c4bbc9d7-65ec-4675-bab8-2e56eb0cfb55"
   },
   "source": [
    "- Based on the model architecture above, we can see that the model now contains our new `LinearWithLoRA` layers\n",
    "- Also, since we initialized matrix $B$ with 0's, we expect the initial model performance to be unchanged compared to before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "DAlrb_I00VEU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DAlrb_I00VEU",
    "outputId": "3da44ac4-230b-4358-d996-30b63f0d962a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13735b3e-f0c3-4dba-ae3d-4141b2878101",
   "metadata": {
    "id": "13735b3e-f0c3-4dba-ae3d-4141b2878101"
   },
   "source": [
    "- Let's now get to the interesting part and finetune the model by reusing the training function from chapter 6\n",
    "- The training takes about 15 minutes on a M3 MacBook Air laptop computer and less than half a minute on a V100 or A100 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "wCParRvr0eff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wCParRvr0eff",
    "outputId": "ce910a9c-ee89-48bb-bfa6-49c6aee1e450"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 3.820, Val loss 3.462\n",
      "Ep 1 (Step 000050): Train loss 0.346, Val loss 0.325\n",
      "Ep 1 (Step 000100): Train loss 0.063, Val loss 0.144\n",
      "Training accuracy: 100.00% | Validation accuracy: 92.50%\n",
      "Ep 2 (Step 000150): Train loss 0.054, Val loss 0.045\n",
      "Ep 2 (Step 000200): Train loss 0.058, Val loss 0.122\n",
      "Ep 2 (Step 000250): Train loss 0.041, Val loss 0.199\n",
      "Training accuracy: 100.00% | Validation accuracy: 95.00%\n",
      "Ep 3 (Step 000300): Train loss 0.020, Val loss 0.153\n",
      "Ep 3 (Step 000350): Train loss 0.018, Val loss 0.191\n",
      "Training accuracy: 100.00% | Validation accuracy: 95.00%\n",
      "Ep 4 (Step 000400): Train loss 0.019, Val loss 0.088\n",
      "Ep 4 (Step 000450): Train loss 0.001, Val loss 0.045\n",
      "Ep 4 (Step 000500): Train loss 0.001, Val loss 0.141\n",
      "Training accuracy: 100.00% | Validation accuracy: 95.00%\n",
      "Ep 5 (Step 000550): Train loss 0.007, Val loss 0.106\n",
      "Ep 5 (Step 000600): Train loss 0.000, Val loss 0.314\n",
      "Training accuracy: 100.00% | Validation accuracy: 92.50%\n",
      "Training completed in 31.14 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from previous_chapters import train_classifier_simple\n",
    "# Alternatively:\n",
    "# from llms_from_scratch.ch06 import train_classifier_simple\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=8e-4, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c89e82-3aa8-44c6-b046-0b16200b8e6c",
   "metadata": {
    "id": "d0c89e82-3aa8-44c6-b046-0b16200b8e6c"
   },
   "source": [
    "- Finally, let's evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bawWGijA0iF3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "bawWGijA0iF3",
    "outputId": "af70782a-d605-4376-fa6c-d33b38979cfa"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from previous_chapters import plot_values\n",
    "# Alternatively:\n",
    "# from llms_from_scratch.ch06 import plot_values\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses, label=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa074723-e3f7-4f7e-a267-855531a037dc",
   "metadata": {
    "id": "aa074723-e3f7-4f7e-a267-855531a037dc"
   },
   "source": [
    "- Note that we previously calculated the accuracy values on 5 batches only via the `eval_iter=5` setting; below, we calculate the accuracies on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1D2awlEq0gZi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1D2awlEq0gZi",
    "outputId": "d603eda1-d912-43eb-ec9c-af6a622510a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 98.37%\n",
      "Validation accuracy: 95.97%\n",
      "Test accuracy: 95.33%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f87f5e6-339e-4fcf-900b-6d845d3c713d",
   "metadata": {
    "id": "1f87f5e6-339e-4fcf-900b-6d845d3c713d"
   },
   "source": [
    "- As we can see based on the relatively high accuracy values above, the LoRA finetuning was successful"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
