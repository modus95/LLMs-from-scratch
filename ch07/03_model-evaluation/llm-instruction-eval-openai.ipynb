{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136a4efe-fb99-4311-8679-e0a5b6282755",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1910a06-e8a3-40ac-8201-ff70615b1ba4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluating Instruction Responses Using the OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128651b-f326-4232-a994-42f38b7ed520",
   "metadata": {},
   "source": [
    "- This notebook uses OpenAI's GPT-4 API to evaluate responses by a instruction finetuned LLMs based on an dataset in JSON format that includes the generated model responses, for example:\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"instruction\": \"What is the atomic number of helium?\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"The atomic number of helium is 2.\",               # <-- The target given in the test set\n",
    "    \"model 1 response\": \"\\nThe atomic number of helium is 2.0.\", # <-- Response by an LLM\n",
    "    \"model 2 response\": \"\\nThe atomic number of helium is 3.\"    # <-- Response by a 2nd LLM\n",
    "},\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "267ba0d1-b884-42df-85bd-0be746fd47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -r requirements-extra.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63610acc-db94-437f-8d38-e99dca0299cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai version: 2.0.1\n",
      "tqdm version: 4.67.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"openai\",  # OpenAI API\n",
    "        \"tqdm\",    # Progress bar\n",
    "        ]\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdcb34-ac75-4f4f-9505-3ce0666c42d5",
   "metadata": {},
   "source": [
    "## Test OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558a522-650d-401a-84fc-9fd7b1f39da7",
   "metadata": {},
   "source": [
    "- First, let's test if the OpenAI API is correctly set up\n",
    "- If you don't have an account yet, you need to create one at https://platform.openai.com/\n",
    "- Note that you will also have to transfer some funds to your account as the GPT-4 API is not free (see https://platform.openai.com/settings/organization/billing/overview)\n",
    "- Running the experiments and creating the ~200 evaluations using the code in this notebook costs about $0.26 (26 cents) as of this writing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89343a84-0ddc-42fc-bf50-298a342b93c0",
   "metadata": {},
   "source": [
    "- First, we need to provide our OpenAI API secret key, which can be found at https://platform.openai.com/api-keys\n",
    "- Make sure not to share this key with anyone\n",
    "- Add this secret key (`\"sk-...\"`) to the `config.json` file in this folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b0ba76-1fb1-4306-a7c2-8f3bb637ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b628ed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load API key from a JSON file.\n",
    "# Make sure to replace \"sk-...\" with your actual API key from https://platform.openai.com/api-keys\n",
    "with open(\"config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "    api_key = config[\"OPENAI_API_KEY\"]\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16642a48-1cab-40d2-af08-ab8c2fbf5876",
   "metadata": {},
   "source": [
    "- First, let's try the API with a simple example to make sure it works as intended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e9ef2e-e816-4283-840e-43625791ad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chatgpt(prompt, client, model=\"gpt-5-mini\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=1,\n",
    "        seed=123,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1b586f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Respond with 'hello world' if you got this message.\"\n",
    "run_chatgpt(prompt, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a4739-6f03-4092-a5c2-f57a0b6a4c4d",
   "metadata": {},
   "source": [
    "## Load JSON Entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca011a8b-20c5-4101-979e-9b5fccf62f8a",
   "metadata": {},
   "source": [
    "- Here, we assume that we saved the test dataset and the model responses as a JSON file that we can load as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b2d393a-aa92-4190-9d44-44326a6f699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 110\n"
     ]
    }
   ],
   "source": [
    "json_file = \"instruction-data-with-response_74.json\"\n",
    "# json_file = \"eval-example-data.json\"\n",
    "\n",
    "with open(json_file, \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "print(\"Number of entries:\", len(json_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c9751b-59b7-43fe-acc7-14e8daf2fa66",
   "metadata": {},
   "source": [
    "- The structure of this file is as follows, where we have the given response in the test dataset (`'output'`) and responses by two different models (`'model 1 response'` and `'model 2 response'`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7222fdc0-5684-4f2b-b741-3e341851359e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Rewrite the sentence using a simile.',\n",
       " 'input': 'The car is very fast.',\n",
       " 'output': 'The car is as fast as lightning.',\n",
       " 'model_response': 'The car is as fast as an elephant.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf0331b-6024-4bba-89a9-a088b14a1046",
   "metadata": {},
   "source": [
    "- Below is a small utility function that formats the input for visualization purposes later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43263cd3-e5fb-4ab5-871e-3ad6e7d21a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. Write a response that \"\n",
    "        f\"appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    instruction_text + input_text\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f953bd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n"
     ]
    }
   ],
   "source": [
    "print(format_input(json_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a55283-7d51-4136-ba60-f799d49f4098",
   "metadata": {},
   "source": [
    "- Now, let's try the OpenAI API to compare the model responses (we only evaluate the first 5 responses for a visual comparison):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "735cc089-d127-480a-b39d-0782581f0c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> Why was the math book sad? Because it had too many problems!\n",
      "\n",
      "Model response:\n",
      ">> The story begins with a man who goes to the market every Sunday. His wife goes to the market every Sunday, too.\n",
      "\n",
      "Score:\n",
      ">> 10\n",
      "\n",
      "Reason: The response is coherent but not funny and fails to deliver a humorous anecdote or punchline. It's repetitive and incomplete for the requested task.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The correct spelling is 'receive'.\n",
      "\n",
      "Model response:\n",
      ">> The correct spelling is 'receive'.\n",
      "\n",
      "Score:\n",
      ">> 100 — The response correctly and concisely identifies the right spelling (\"receive\") and directly answers the instruction.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> Nostalgia washed over her as she looked through the old photos.\n",
      "\n",
      "Model response:\n",
      ">> She was very nostalgic for the past.\n",
      "\n",
      "Score:\n",
      ">> Score: 25/100\n",
      "\n",
      "Reasoning:\n",
      "- The response is a grammatically fine sentence and conveys the intended meaning, but it fails the explicit instruction: the user asked to use the word \"nostalgia\" (the noun), while the model used \"nostalgic\" (the adjective).  \n",
      "- Because it does not follow the core requirement, it receives a low score despite being natural and relevant.\n",
      "\n",
      "Suggested correct version: \"Nostalgia washed over her as she looked through the old photos.\"\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for entry in json_data[6:9]:\n",
    "    # prompt = (f\"Given the input `{format_input(entry)}` \"\n",
    "    #           f\"and correct output `{entry['output']}`, \"\n",
    "    #           f\"score the model response `{entry['model_response']}`\"\n",
    "    #           f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "    #           )\n",
    "    prompt = f\"\"\"\n",
    "Given the model input:\n",
    "\"{format_input(entry)}\"\n",
    "\n",
    "and correct model output: \"{entry['output']}\"\n",
    "\n",
    "Score the model response \"{entry['model_response']}\" on a scale from 0 to 100, where 100 is the best score.\n",
    "\"\"\"\n",
    "    \n",
    "    # print(prompt)\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry[\"model_response\"])\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", run_chatgpt(prompt, client))\n",
    "    print(\"\\n-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba1052de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for entry in json_data[:5]:\n",
    "#     prompt = (f\"Given the input `{format_input(entry)}` \"\n",
    "#               f\"and correct output `{entry['output']}`, \"\n",
    "#               f\"score the model response `{entry['model 1 response']}`\"\n",
    "#               f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "#               )\n",
    "#     print(\"\\nDataset response:\")\n",
    "#     print(\">>\", entry['output'])\n",
    "#     print(\"\\nModel response:\")\n",
    "#     print(\">>\", entry[\"model 1 response\"])\n",
    "#     print(\"\\nScore:\")\n",
    "#     print(\">>\", run_chatgpt(prompt, client))\n",
    "#     print(\"\\n-------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142dfaa7-429f-4eb0-b74d-ff327f79547a",
   "metadata": {},
   "source": [
    "- Note that the responses are very verbose; to quantify which model is better, we only want to return the scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3552bdfb-7511-42ac-a9ec-da672e2a5468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def generate_model_scores(json_data, client):\n",
    "    scores = []\n",
    "    \n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        # prompt = (\n",
    "        #     f\"Given the input `{format_input(entry)}` \"\n",
    "        #     f\"and correct output `{entry['output']}`, \"\n",
    "        #     f\"score the model response\"\n",
    "        #     f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "        #     f\"Respond with the number only.\"\n",
    "        # )\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Given the model input:\n",
    "\"{format_input(entry)}\"\n",
    "\n",
    "and correct model output: \"{entry['output']}\"\n",
    "\n",
    "Score the model response \"{entry['model_response']}\" on a scale from 0 to 100, where 100 is the best score.\n",
    "Respond with the number only.\n",
    "\"\"\"\n",
    "        score = run_chatgpt(prompt, client)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc78c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "# def generate_model_scores(json_data, json_key, client):\n",
    "#     scores = []\n",
    "#     for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "#         prompt = (\n",
    "#             f\"Given the input `{format_input(entry)}` \"\n",
    "#             f\"and correct output `{entry['output']}`, \"\n",
    "#             f\"score the model response `{entry[json_key]}`\"\n",
    "#             f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "#             f\"Respond with the number only.\"\n",
    "#         )\n",
    "#         score = run_chatgpt(prompt, client)\n",
    "#         try:\n",
    "#             scores.append(int(score))\n",
    "#         except ValueError:\n",
    "#             continue\n",
    "\n",
    "#     return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71974dea-31ed-49af-abba-5c858bbbf49c",
   "metadata": {},
   "source": [
    "- Please note that the response scores may vary because OpenAI's GPT models are not deterministic despite setting a random number seed, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b071ce84-1866-427f-a272-b46700f364b2",
   "metadata": {},
   "source": [
    "- Let's now apply this evaluation to the whole dataset and compute the average score of each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f700d4b-19e5-4404-afa7-b0f093024232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|██████████| 110/110 [06:58<00:00,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 110 of 110\n",
      "Average score: 33.01\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from pathlib import Path\n",
    "# Exercise 7.4 (LoRA)\n",
    "\n",
    "scores = generate_model_scores(json_data, client)\n",
    "print(f\"Number of scores: {len(scores)} of {len(json_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")\n",
    "\n",
    "    # Optionally save the scores\n",
    "    # save_path = Path(\"scores\") / f\"gpt4-{model.replace(' ', '-')}.json\"\n",
    "    # with open(save_path, \"w\") as file:\n",
    "    #     json.dump(scores, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab86b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|██████████| 110/110 [08:15<00:00,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 110 of 110\n",
      "Average score: 32.07\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from pathlib import Path\n",
    "# Exercise 7.2 (marked input instructions)\n",
    "\n",
    "scores = generate_model_scores(json_data, client)\n",
    "print(f\"Number of scores: {len(scores)} of {len(json_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")\n",
    "\n",
    "    # Optionally save the scores\n",
    "    # save_path = Path(\"scores\") / f\"gpt4-{model.replace(' ', '-')}.json\"\n",
    "    # with open(save_path, \"w\") as file:\n",
    "    #     json.dump(scores, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe853ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|██████████| 110/110 [06:45<00:00,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 110 of 110\n",
      "Average score: 33.39\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from pathlib import Path\n",
    "# Exercise 7.1 (Phi3)\n",
    "\n",
    "scores = generate_model_scores(json_data, client)\n",
    "print(f\"Number of scores: {len(scores)} of {len(json_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")\n",
    "\n",
    "    # Optionally save the scores\n",
    "    # save_path = Path(\"scores\") / f\"gpt4-{model.replace(' ', '-')}.json\"\n",
    "    # with open(save_path, \"w\") as file:\n",
    "    #     json.dump(scores, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d568fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|██████████| 110/110 [06:29<00:00,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 110 of 110\n",
      "Average score: 99.45\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from pathlib import Path\n",
    "# Original\n",
    "\n",
    "scores = generate_model_scores(json_data, client)\n",
    "print(f\"Number of scores: {len(scores)} of {len(json_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")\n",
    "\n",
    "    # Optionally save the scores\n",
    "    # save_path = Path(\"scores\") / f\"gpt4-{model.replace(' ', '-')}.json\"\n",
    "    # with open(save_path, \"w\") as file:\n",
    "    #     json.dump(scores, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cc304c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 40, 80, 20, 0, 0, 0, 100, 10, 100, 100, 0, 0, 0, 0, 0, 0, 100, 0, 0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 7.1\n",
    "scores[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceb904f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 60, 100, 10, 0, 0, 0, 100, 0, 95, 100, 70, 0, 0, 0, 0, 10, 0, 0, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 7.2\n",
    "scores[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90f0ec24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 70, 50, 0, 0, 15, 0, 0, 10, 90, 80, 80, 10, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 7.4 (LoRA)\n",
    "scores[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fce885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|████████████████████████| 100/100 [01:03<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model 1 response\n",
      "Number of scores: 100 of 100\n",
      "Average score: 74.09\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|████████████████████████| 100/100 [01:06<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model 2 response\n",
      "Number of scores: 100 of 100\n",
      "Average score: 56.57\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# for model in (\"model 1 response\", \"model 2 response\"):\n",
    "\n",
    "#     scores = generate_model_scores(json_data, model, client)\n",
    "#     print(f\"\\n{model}\")\n",
    "#     print(f\"Number of scores: {len(scores)} of {len(json_data)}\")\n",
    "#     print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")\n",
    "\n",
    "#     # Optionally save the scores\n",
    "#     save_path = Path(\"scores\") / f\"gpt4-{model.replace(' ', '-')}.json\"\n",
    "#     with open(save_path, \"w\") as file:\n",
    "#         json.dump(scores, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8169d534-1fec-43c4-9550-5cb701ff7f05",
   "metadata": {},
   "source": [
    "- Based on the evaluation above, we can say that the 1st model is substantially better than the 2nd model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
